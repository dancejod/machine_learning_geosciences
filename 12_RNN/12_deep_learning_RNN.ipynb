{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "\n",
    "# Machine Learning in Geosciences ] \n",
    "Department of Applied Geoinformatics and Carthography, Charles University\n",
    "\n",
    "Lukas Brodsky lukas.brodsky@natur.cuni.cz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Recurrent Neural Network\n",
    "\n",
    "This notebooks introduces solutions of problems when we're trying to explain changes over time. Situation when a predicted value depends on a series of past behaviors. \n",
    "\n",
    "This challenge of incorporating a series of measurements over time into the model parameters is addressed by Recurrent Neural Networks. \n",
    "\n",
    "**PyTorch** offers a number of RNN layers and options.<br>\n",
    "\n",
    "* <a href='https://pytorch.org/docs/stable/nn.html#rnn'><tt><strong>torch.nn.RNN()</strong></tt></a> provides a basic model which applies a multilayer RNN with either <em>tanh</em> or <em>ReLU</em> non-linearity functions to an input sequence.<br>\n",
    "As we learned in the theory lectures, however, this has its limits.<br><br>\n",
    "* <a href='https://pytorch.org/docs/stable/nn.html#lstm'><tt><strong>torch.nn.LSTM()</strong></tt></a> adds a multi-layer long short-term memory (LSTM) process which greatly extends the memory of the RNN.\n",
    "\n",
    "To demonstrate the potential of LSTMs, we'll look at a simple sine wave. \n",
    "**Goal**: given a value, predict the next value in the sequence. Due to the cyclical nature of sine waves, an typical neural network won't know if it should predict upward or downward, while an LSTM is capable of learning patterns of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periodic data set \n",
    "For this exercise we'll look at a simple sine wave. We'll take **800 data points** and assign **40 points per full cycle**, for a total of **20 complete cycles**. We'll train our model on all but the last cycle, and use that to evaluate our test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create & plot data points\n",
    "x = torch.linspace(0,799,steps=800)\n",
    "y = torch.sin(x*2*3.1416/40)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlim(-10,801)\n",
    "plt.grid(True)\n",
    "plt.plot(y.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test sets\n",
    "We want to take the first 760 samples in our series as a training sequence, and the last 40 for testing.\n",
    "\n",
    "NOTE:  We tend to use the terms \"series\" and \"sequence\" interchangeably. Usually \"series\" refers to the entire population of data, or the full time series, and \"sequence\" refers to some portion of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 40\n",
    "\n",
    "train_set = y[:-test_size]\n",
    "test_set = y[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.xlim(-10,801)\n",
    "plt.grid(True)\n",
    "plt.plot(y.numpy(), 'w');\n",
    "plt.plot(train_set.numpy(), 'b')\n",
    "plt.plot(torch.linspace(train_set.size()[0],799,steps=40), test_set.numpy(), 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training data!!! \n",
    "\n",
    "When working with LSTM models, we start by **dividing the training sequence into a series of overlapping \"windows\"**. Each window consists of a **connected sequence of samples**. The **label used for comparison is equal to the next value in the sequence**. \n",
    "\n",
    "Think of moving window!!!\n",
    "\n",
    "In this way our **network learns what value should follow a given pattern of preceding values**. \n",
    "\n",
    "For example, we have a series of 15 records, and a window size of 5. We feed $[x_1,..,x_5]$ into the model, and compare the prediction to $x_6$. Then we backpropagate, update parameters, and feed $[x_2,..,x_6]$ into the model. We compare the new output to $x_7$ and so forth up to $[x_{10},..,x_{14}]$.\n",
    "\n",
    "To simplify this, we'll define a function called `input_data` that builds a list of <tt>(seq, label)</tt> tuples. Windows overlap, so the first tuple might contain $([x_1,..,x_5],[x_6])$, the second would have $([x_2,..,x_6],[x_7])$, etc. \n",
    "\n",
    "Here $k$ is the width of the window. Due to the overlap, we'll have a total number of <tt>(seq, label)</tt> tuples equal to $\\textrm{len}(series)-k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(seq,ws):  \n",
    "    \"\"\"seq .. input sequence \n",
    "       ws .. the window size\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    L = len(seq)\n",
    "    for i in range(L-ws):\n",
    "        window = seq[i:i+ws]\n",
    "        label = seq[i+ws:i+ws+1]\n",
    "        out.append((window,label))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \"Windows\" are different from \"batches\". **In our example we'll feed one window into the model at a time**, so our batch size would be 1. If we passed two windows into the model before we backprop and update weights, our batch size would be 2.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train on our sine wave data we'll use a window size of 40 (one entire cycle). \n",
    "\n",
    "# test_size = 40\n",
    "# train_set = y[:-test_size]\n",
    "# test_set = y[-test_size:]\n",
    "\n",
    "window_size = 40\n",
    "\n",
    "# Create the training dataset of sequence/label tuples:\n",
    "train_data = input_data(train_set, window_size)\n",
    "\n",
    "len(train_data) # this should equal 760-40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Window data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the first (seq/label) tuple in train_data\n",
    "torch.set_printoptions(sci_mode=False) # to improve the appearance of tensors\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.xlim(-1,43)\n",
    "plt.grid(True)\n",
    "plt.plot(train_data[0][0].numpy(), 'b*--')\n",
    "plt.plot(torch.tensor(40), train_data[0][1].numpy(), 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.xlim(-1,43)\n",
    "plt.grid(True)\n",
    "plt.plot(torch.linspace(1,40,steps=40), train_data[1][0].numpy(), 'b*--')\n",
    "plt.plot(torch.tensor(41), train_data[1][1].numpy(), 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.xlim(-1,43)\n",
    "plt.grid(True)\n",
    "plt.plot(torch.linspace(2,41,steps=40), train_data[2][0].numpy(), 'b*--')\n",
    "plt.plot(torch.tensor(42), train_data[2][1].numpy(), 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and so on! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "* one LSTM layer    \n",
    "* input size of 1     \n",
    "* hidden size of 50 (can be changed)     \n",
    "* outpus as a fully-connected layer to reduce the output to the prediction size of 1.<br>\n",
    "    \n",
    "NOTE: You will often see the terms `input_dim` and `hidden_dim` used in place of `input_size` and `hidden_size`. They mean the same thing. Let's stick to `input_size` and `hidden_size` to stay consistent with PyTorch's built-in keywords.\n",
    "\n",
    "During training we pass three tensors through the LSTM layer - the sequence, the hidden state **$h_0$** and the cell state **$c_0$**.\n",
    "\n",
    "This means we need to initialize $h_0$ and $c_0$. This can be done with random values, but we'll use zeros instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, out_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # LSTM layer:\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        \n",
    "        # Fully-connected layer:\n",
    "        self.linear = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "        # Initialize h0 and c0:\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size),\n",
    "                       torch.zeros(1, 1, hidden_size))\n",
    "    \n",
    "    def forward(self,seq):\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            seq.view(len(seq), 1, -1), self.hidden)\n",
    "        pred = self.linear(lstm_out.view(len(seq),-1))\n",
    "\n",
    "        # return only the last prediction\n",
    "        return pred[-1]   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation \n",
    "\n",
    "* instantiate the model, \n",
    "* define loss \n",
    "* an d optimization functions\n",
    "\n",
    "Since we're comparing single values, we'll use MSEloss: https://pytorch.org/docs/stable/nn.html#mseloss torch.nn.MSELoss.  \n",
    "\n",
    "Also, we've found that https://pytorch.org/docs/stable/optim.html#torch.optim.SGD torch.optim.SGD converges faster for this application than torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# instnce of outr model\n",
    "model = LSTM()\n",
    "# model = RNN()\n",
    "# optimisation criterion \n",
    "criterion = nn.MSELoss()\n",
    "# optimizer \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hidden[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [p.numel() for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on 10 epochs \n",
    "epochs = 10\n",
    "future = 40\n",
    "loss_history = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # tuple-unpack the train_data set\n",
    "    for seq, y_train in train_data:\n",
    "        \n",
    "        # reset the parameters and hidden states\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        \n",
    "        y_pred = model(seq)\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print training result\n",
    "    print(f'Epoch: {i+1:2} Loss: {loss.item():10.8f}')\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "# print('Training finished!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting future values\n",
    "To show how an LSTM model improves after each epoch, we'll run predictions and plot the results. Our goal is to predict the last sequence of 40 values, and compare them to the known data in our test set. However, we have to be careful <em>not</em> to use test data in the predictions - that is, each new prediction derives from previously predicted values.\n",
    "\n",
    "The task is to take the last known window, predict the next value, then <em>append</em> the predicted value to the sequence and run a new prediction on a window that includes the value we've just predicted. \n",
    "In this way, a well-trained model <em>should</em> follow any regular trends/cycles in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and simultaneously evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = train_set[-window_size:].tolist()\n",
    "# preds\n",
    "seq = torch.FloatTensor(preds[-window_size:])\n",
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the training approach, the gradient is propagated through the hidden states of the LSTM across the time dimension in the batch.\n",
    "\n",
    "The hidden state stores the internal state of the RNN from predictions made on previous tokens in the current sequence, this allows RNNs to understand context. The hidden state is determined by the output of the previous token.\n",
    "\n",
    "When you predict for the first token of any sequence, if you were to retain the hidden state from the previous sequence your model would perform as if the new sequence was a continuation of the old sequence which would give worse results. Instead for the first token you initialise an empty hidden state, which will then be filled with the model state and used for the second token. \n",
    "\n",
    "Consider hidden states as just outputs, which are not updated during backprop. So, for every new epoch and (every iteration) we re-initialise hidden_state vectors, so as to compute hidden_state vectors for each sequence individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We'll train 10 epochs. For clarity, we'll \"zoom in\" on the test set, \n",
    "# and only display from point 700 to the end.\n",
    "epochs = 10\n",
    "future = 40\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # tuple-unpack the train_data set\n",
    "    for seq, y_train in train_data:\n",
    "        \n",
    "        # reset the parameters and hidden states\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        \n",
    "        y_pred = model(seq)\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print training result\n",
    "    print(f'Epoch: {i+1:2} Loss: {loss.item():10.8f}')\n",
    "    \n",
    "    # MAKE PREDICTIONS after training in each epoch \n",
    "    # start with a list of the last 40 training records\n",
    "    preds = train_set[-window_size:].tolist()\n",
    "    print(len(preds))\n",
    "\n",
    "    for f in range(future):  \n",
    "        seq = torch.FloatTensor(preds[-window_size:])\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                            torch.zeros(1,1,model.hidden_size))\n",
    "            preds.append(model(seq).item())\n",
    "            # print(len(preds))\n",
    "            \n",
    "    loss = criterion(torch.tensor(preds[-window_size:]),y[760:])\n",
    "    print(f'Loss on test predictions: {loss}')\n",
    "\n",
    "    # Plot from point 700 to the end\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.xlim(700,801)\n",
    "    plt.grid(True)\n",
    "    plt.plot(y.numpy())\n",
    "    plt.plot(range(760,800),preds[window_size:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting into an unknown future\n",
    "We'll continue to train our model, this time using the entire dataset. Then we'll predict what the <em>next</em> 40 points should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Expect this to take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "window_size = 40\n",
    "future = 40\n",
    "\n",
    "# Create the full set of sequence/label tuples:\n",
    "all_data = input_data(y,window_size)\n",
    "len(all_data)  # this should equal 800-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # tuple-unpack the entire set of data\n",
    "    for seq, y_train in all_data:  \n",
    "       \n",
    "        # reset the parameters and hidden states\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        \n",
    "        y_pred = model(seq)\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print training result\n",
    "    print(f'Epoch: {i+1:2} Loss: {loss.item():10.8f}')\n",
    "    \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict future values, plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = y[-window_size:].tolist()\n",
    "\n",
    "for i in range(future):  \n",
    "    seq = torch.FloatTensor(preds[-window_size:])\n",
    "    with torch.no_grad():\n",
    "        # Reset the hidden parameters\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))  \n",
    "        preds.append(model(seq).item())\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlim(-10,841)\n",
    "plt.grid(True)\n",
    "plt.plot(y.numpy())\n",
    "plt.plot(range(800,800+future),preds[window_size:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
