{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning in Geosciences -  Ensemble learning\n",
    "\n",
    "**Department of Applied Geoinformatics and Carthography, Charles University** \n",
    "\n",
    "03.04.2023\n",
    "\n",
    "---------------\n",
    "Mgr. Daniel Bic√°k    \n",
    "*bicakd@natur.cuni.cz*\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to create a dataset, which will be used for testing our algorithms. We will use **scipy** library. You can install it using `conda install -c anaconda scipy`. Our dataset will consist of **200 samples** (instances) and has **4 features**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function for min max scalling\n",
    "\n",
    "def custom_scaler(data, min_v, max_v):\n",
    "    return min_v+(((data-data.min())*(max_v-min_v))/(data.max()-data.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset size\n",
    "dataset_size = 200\n",
    "\n",
    "# generate all features, we will sample it from logistic distribution\n",
    "from scipy.stats import logistic\n",
    "\n",
    "feature1 = logistic.rvs(size=dataset_size).reshape(-1,1)\n",
    "feature2 = logistic.rvs(size=dataset_size).reshape(-1,1)\n",
    "feature3 = logistic.rvs(size=dataset_size).reshape(-1,1)\n",
    "feature4 = logistic.rvs(size=dataset_size).reshape(-1,1)\n",
    "\n",
    "\n",
    "# scale features\n",
    "feature1 = custom_scaler(feature1, 0,1)\n",
    "feature2 = custom_scaler(feature2, 0,1)\n",
    "feature3 = custom_scaler(feature3, 0,1)\n",
    "feature4 = custom_scaler(feature4, 0,10)\n",
    "\n",
    "\n",
    "# create relationship\n",
    "feature1_v = 1.2 * feature1\n",
    "feature2_v = (-2*(feature2**2)) + (2*feature2)\n",
    "feature3_v = (3*(feature3**2)) + (feature3)\n",
    "feature4_v = np.sin(feature4)\n",
    "\n",
    "# calculate the dependent variable\n",
    "D = (feature1_v+feature2_v+feature3_v+feature4_v).flatten()\n",
    "\n",
    "# inject the noise\n",
    "# noise has max size of 10% of max value of D\n",
    "max_size = D.max()/10\n",
    "\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# we will generate it from uniform distribution and randomly add to D\n",
    "noise = uniform.rvs(size=dataset_size)\n",
    "\n",
    "# scale the noise from -max to max\n",
    "noise_scaled = custom_scaler(noise, -max_size, max_size)\n",
    "\n",
    "# add to the dependent variable\n",
    "D_with_noise = D + noise_scaled\n",
    "\n",
    "# scale back feature4\n",
    "feature4 = custom_scaler(feature4, 0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,4))\n",
    "\n",
    "fig1ax1 = fig1.add_subplot(121)\n",
    "fig1ax2 = fig1.add_subplot(122)\n",
    "\n",
    "fig1ax1.scatter(feature1, D_with_noise, s=3)\n",
    "fig1ax1.set_title('feature 1')\n",
    "fig1ax2.scatter(feature4, D_with_noise, s=3)\n",
    "fig1ax2.set_title('feature 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(6,4))\n",
    "\n",
    "fig2ax1 = fig2.add_subplot(111)\n",
    "fig2ax1.hist(D_with_noise, bins=20)\n",
    "fig2ax1.set_title('Dependent variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no visible patterns in the data. Can our algorithms approximate relationship accurately? Let's find out! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# merge all array into one\n",
    "Dataset = np.hstack((feature1, feature2, feature3, feature4))\n",
    "\n",
    "# split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(Dataset, D_with_noise, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest and Decision trees\n",
    "\n",
    "`scikit learn` contains Random Forest class object. We can work with regressor similarly to other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a Random Forest instance\n",
    "RF_reg = RandomForestRegressor(max_depth=10, random_state=42, n_estimators=200, max_features=3)\n",
    "RF_reg.fit(X_train, y_train)\n",
    "\n",
    "# Create a Decision tree instance\n",
    "DT_reg = DecisionTreeRegressor(random_state=42)\n",
    "DT_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# make a prediction for each algorithm\n",
    "RF_pred = RF_reg.predict(X_test)\n",
    "RF_rmse = mean_squared_error(y_test, RF_pred, squared=False)\n",
    "\n",
    "DT_pred = DT_reg.predict(X_test)\n",
    "DT_rmse = mean_squared_error(y_test, DT_pred, squared=False)\n",
    "\n",
    "print(f'RMSE for Random Forest is: {RF_rmse}, and for Decision Tree is: {DT_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the importance value for each feature? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore how can Random Forest approximate the relationship. We will generate values for desired feature, other will be set to mean value. Let's investigate the relationship between feature4 and dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate array from 0 to 1, dummy values for feature we want to explore\n",
    "size=500\n",
    "\n",
    "desired_feature = np.linspace(0,1,size)\n",
    "\n",
    "# other features can have random single value, however best suited is mean value\n",
    "feat1_mean = np.full(shape=size, fill_value=0, dtype=float)\n",
    "feat2_mean = np.full(shape=size, fill_value=0, dtype=float)\n",
    "feat3_mean = np.full(shape=size, fill_value=0, dtype=float)\n",
    "\n",
    "# merge features\n",
    "RF_test_set = np.hstack((feat1_mean.reshape(-1,1), feat2_mean.reshape(-1,1), feat3_mean.reshape(-1,1), desired_feature.reshape(-1,1)))\n",
    "\n",
    "# and predict\n",
    "RF_output = RF_reg.predict(RF_test_set)\n",
    "\n",
    "# we can do the same for decision tree\n",
    "DT_output = DT_reg.predict(RF_test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the output\n",
    "\n",
    "fig3 = plt.figure(figsize=(14,6))\n",
    "\n",
    "fig3ax1 = fig3.add_subplot(121)\n",
    "fig3ax1.scatter(desired_feature, RF_output, s=2)\n",
    "fig3ax1.set_title('Random Forest')\n",
    "fig3ax1.plot(desired_feature, np.sin(custom_scaler(desired_feature, 0, 10)), c='r' )\n",
    "\n",
    "\n",
    "fig3ax2 = fig3.add_subplot(122)\n",
    "fig3ax2.scatter(desired_feature, DT_output, s=2, c='r')\n",
    "fig3ax2.set_title('Decision Tree')\n",
    "fig3ax2.plot(desired_feature, (custom_scaler(np.sin(custom_scaler(desired_feature, 0, 10)), 0,1)), c='b' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Can you tune the hyperparameters of these two algorithms? Try to achieve better results! You can use *Out-of-bag* samples when tuning Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Plot the parameter *number of trees* and RMSE of Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost and Gradient Boost\n",
    "\n",
    "An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.\n",
    "\n",
    "Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions, see the seminal work of J.H. Friedman. GBDT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems in a variety of areas including Web search ranking and ecology.\n",
    "\n",
    "*from scikit learn*\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor\n",
    "\n",
    "https://scikitlearn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "Ada_reg = AdaBoostRegressor(random_state=42, n_estimators=100, learning_rate=1)\n",
    "Grad_reg = GradientBoostingRegressor(random_state=42, learning_rate=0.1)\n",
    "\n",
    "Ada_reg.fit(X_train, y_train)\n",
    "Grad_reg.fit(X_train, y_train)\n",
    "\n",
    "Ada_pred = Ada_reg.predict(X_test)\n",
    "Grad_pred = Grad_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ada_rmse = mean_squared_error(y_test, Ada_pred, squared=False)\n",
    "Grad_rmse = mean_squared_error(y_test, Grad_pred, squared=False)\n",
    "\n",
    "print(f'RMSE for AdaBoost is: {Ada_rmse}, and for Gradient Boosting is: {Grad_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out, what is the best learning rate for Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Read the documentation and apply the *early stopping* option on Gradient Boosting algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging -  Exercise 5\n",
    "\n",
    "Bagging is a general concept. This example will focus on applying bagging to the K-Nearest Neigbour algorithm. KNN is relatively weak algorithm, we want to know, whether bagging can improve the performace. Can KNN match the power of Random Forest or Gradient Boost? We will use the same dataset. Find the information about bagging here; https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# create an instance of regressor\n",
    "KNN_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# find the best possible parameters using gridsearchCV\n",
    "\n",
    "# create an instance of bagging\n",
    "\n",
    "# fit the data\n",
    ".fit(X_train, y_train)\n",
    "\n",
    "# predict new values\n",
    ".predict(X_test)\n",
    "\n",
    "# calculate RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can improve the accuracy further by changing the parameters of bagging\n",
    "# parameter \"max_features\" will induce diversity into the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting - Exercise 6\n",
    "\n",
    "Stack of estimators with a final regressor.\n",
    "\n",
    "Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n",
    "\n",
    "*- scikit learn*\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate all predictor and create a new model. Hypothethically, new model should achieve best results. It is up to you, which and how many of sub-model you include in meta-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
