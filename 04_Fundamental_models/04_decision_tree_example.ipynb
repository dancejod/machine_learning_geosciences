{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___\n",
    "\n",
    "# [ Machine Learning in Geosciences ]\n",
    "\n",
    "**Department of Applied Geoinformatics and Carthography, Charles University** \n",
    "\n",
    "*Lukas Brodsky lukas.brodsky@natur.cuni.cz*\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Machine Learning example Scikit-learn library is used to develop machine learning model for classification problem. Here we will be implementing a simple decision tree model to classify iris plants into three species using the lengths and widths of their petals and sepals, a [classic data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) in the machine learning community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris samples in question fall into three species: *Iris setosa*, *Iris versicolor*, and *Iris virginica*.  At first glance, the three species can look remarkably like each other, and this is especially the case for the latter two.  However, thanks to the careful measurements taken and presented by Anderson and Fisher, an individual sample may be classified with a high degree of accuracy via machine learning models if the relevant measurements are taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem definition:\n",
    "Develop predictive Machine Learning model to predict (classify) Iris specirs based on four measured features: petal length,  sepal length, petal with and sepal petal. \n",
    "(See below picture to know what these represent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning model\n",
    "\n",
    "The decision tree models are the simplest form of tree-based models, and are arguably the simplest form of supervised multivariate classification models. A series of logical tests (generally in the form of boolean comparisons) are applied to the sample entries and their resulting subsets in turn to arrive at a final decision. It is very easy to visualize the decision process in a simple flowchart to trace the rational of every assignment made by a decision tree model, making it among the most interpretable of models. The are called 'white box'type of models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "We will be using the **Scikit-learn library** for the machine Learning example [sklearn](https://scikit-learn.org). It is the most comprehensive library of Machine Learning algoritms. It is simple and efficient library for predictive data analysis, accessible to everybody (Open Source), and reusable in various contexts, not only for scientific work. It built on NumPy, Pandas, SciPy, and matplotlib essential libraries. \n",
    "\n",
    "The following code, strongly derived from [scikit-learn's documentation](https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html), shows how a simple decision tree model may be applied and visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "\n",
    "First we, have to compare the model predicted values with the validation / testing values. These provide us comparison if our model classification result is correct or made a mistake. Typically we plot it in a confusion matrix. Given an samples for two classification categories, we can refer to samples as positive or negative (two\n",
    "classes). \n",
    "\n",
    "TODO: view simple figure. \n",
    "\n",
    "As you can see it compares the actruall classes in our validation / test set with the model prediction. Green means correct, red is flase. Let's suppose an example of binary classification, only two classes.  We evaluate if the classification element was True Positive, True Negative, False Positive and False Negative. \n",
    "\n",
    "**True Positives** (TP) - These are the **correctly predicted positive values** which means that the value of actual class is yes and the value of predicted class is also yes.\n",
    "\n",
    "**True Negatives** (TN) - These are the **correctly predicted negative values** which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n",
    "\n",
    "**False positives** and false negatives, these values occur **when your actual class contradicts with the predicted class**.\n",
    "\n",
    "**False Positives** (FP) – When **actual class is no and predicted class is yes**. \n",
    "\n",
    "**False Negatives** (FN) – When **actual class is yes but predicted class in no**. \n",
    "\n",
    "---\n",
    "\n",
    "**Accuracy** (Overall Accuracy) is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. \n",
    "\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+FP+FN+TN} $$\n",
    "\n",
    "**Precision** - Precision is the **ratio of correctly predicted positive observations to the total predicted positive observations**. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "**Recall (Sensitivity)** is the **ratio of correctly predicted positive observations to the all observations in actual class - yes**. Example: the question recall answers is: Of all the passengers that truly survived the landslide, how many did we label? \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "**F1 Score** is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. \n",
    "\n",
    "$$ F1 Score = 2 \\frac{precision \\cdot recall}{precision + recall}$$\n",
    "\n",
    "TODO: Maleki et al. 2020 (CC Figure)\n",
    "The left rectangle represents positive samples, and the right rectangle represents negative samples. The\n",
    "circle contains all samples predicted as positive. Given the model predictions, each sample can be considered as TP\n",
    "(true positive), TN (true negative), FP (false positive), or FN (false negative).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Python packages imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Jupyter notebook visualization \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Machine Learning \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Decision Tree model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# model performance \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data \n",
    "The iris dataset can be easily downloaded from Seaborn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating DataFrame called df\n",
    "df = sns.load_dataset('iris') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the the data header by pandas method .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view header of the data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the available variables describing the Iris flowers?\n",
    "\n",
    "-  How many?\n",
    "\n",
    "- Which of the columns represent the target variable? \n",
    "\n",
    "- What is the data type of the target?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get basic info about the dataset with `.info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do yourself \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are the missing data for any record? \n",
    "\n",
    "Use `.isnull().any()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do yourself - check missing data records? \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the dataset dimensionality?\n",
    "\n",
    "Use `.shape` method. The same as NumPy uses. See there is not a bracket here. Eghh :-( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do yourself - check data set dimensionality\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the variables mean! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"iris-dataset.png\" style=\"height:500px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nest, let's explore the dataset in the feature space.\n",
    "\n",
    "Use `Seaborn` method `.pairplot()`. Pass df as data parameter and select 'species' attribute for colloring the pair plots. \n",
    "\n",
    "    data=df\n",
    "\n",
    "    hue='species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do yourself - visualize feature space in pairplot\n",
    "# Beware, it may take few seconds to run! \n",
    "\n",
    "sns.pairplot(data=df, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! What do we see? \n",
    "\n",
    "- Is the problem linear? \n",
    "\n",
    "- Which class(es) are simply separable and which classes are more difficult? \n",
    "\n",
    "- Can you guess what level of noise / uncertainty  is in the data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data preparation (for Machine Learning modelling / or other analysis)\n",
    "\n",
    "Boring but extremely importnt step to succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrame object has attribute `columns`. Use `dataframe.columns` to get the columns / attributes names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of out attributes (features)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable are the species, you can get the column like here: \n",
    "df['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare two variables: X and y. \n",
    "\n",
    "    X .. all features\n",
    "    y .. target variable to model ( ane later predict) based on the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = df['species']\n",
    "df1 = df.copy()\n",
    "# remove the 'species' column \n",
    "X = df1.drop('species', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the two \n",
    "print(X.shape, target_variable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rarget variable (y) need to be converted to numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation\n",
    "le = LabelEncoder()\n",
    "target_variable = le.fit_transform(target_variable) \n",
    "target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# species coding? Which one is which? \n",
    "spec_codes  = pd.concat([df['species'], pd.DataFrame(target_variable)], axis=1)\n",
    "\n",
    "for col in spec_codes:\n",
    "    print(spec_codes[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now our y is ok\n",
    "y = target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we ready for modelling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Splitting training and testing datasets\n",
    "\n",
    "Reminder: simple concept to evaluate possible overfittng problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here simple random splitting, you know better ways \n",
    "# 90% for training, rest for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.4, random_state = 42)\n",
    "\n",
    "print(\"Training set \", X_train.shape)\n",
    "print(\"Testing set \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Machine Learning model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctreate instance of the Decition Tree\n",
    "tree = DecisionTreeClassifier(max_depth=5, min_samples_split=3, random_state = 42) \n",
    "\n",
    "# the hyperparametrs: max. depth, min. samples, splitter are here set by the user to these values\n",
    "# one can use Grid Search or Random Search techniques to finetune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training cross-validation\n",
    "cv_tree = cross_validate(tree, X_train, y_train, cv=3, scoring='f1_weighted', return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prumer vazeneho F1-skore: {:.3f} a std {:.3f}'.format(\n",
    "        cv_tree['test_score'].mean(),\n",
    "        cv_tree['test_score'].std())\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first run model predictions on separate test set!\n",
    "y_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder\n",
    "for col in spec_codes:\n",
    "    print(spec_codes[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix?  A bit more interpretable result \n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(data=cm,linewidths=.5, annot=True, square = True, cmap = 'Blues')\n",
    "plt.ylabel('Reference')\n",
    "plt.xlabel('Predicted class')\n",
    "all_sample_title = 'F1-score: {0}'.format(round(f1_score(y_test, y_pred, average='weighted'), 3))\n",
    "plt.title(all_sample_title, size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Howabout stratified sampling? \n",
    "from sklearn.model_selection import KFold\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in kf5.split(X):\n",
    "    print(\"Running {} loop\".format(i))\n",
    "    # splitting \n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "        \n",
    "    # model training\n",
    "    tree.fit(X_train, y_train) \n",
    "    print(\"Accuracy for fold no. {} on test set: {}\".format(i, f1_score(y_test, tree.predict(X_test), average='weighted')))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model visualization (interpretable machine learning) \n",
    "\n",
    "Remeber Tree-based models are not blackboxes but whiteboxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model \n",
    "figure(figsize=(13, 7), dpi=80)\n",
    "my_decision_tree = plot_tree(decision_tree=tree, feature_names = df1.columns,\n",
    "class_names =[\"setosa\", \"vercicolor\", \"verginica\"] , filled=True ,  precision=2, rounded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
