{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f933ae",
   "metadata": {},
   "source": [
    "# ___\n",
    "\n",
    "# [ Machine Learning in Geosciences ]\n",
    "\n",
    "**Department of Applied Geoinformatics and Carthography, Charles University** \n",
    "\n",
    "*Lukas Brodsky lukas.brodsky@natur.cuni.cz*\n",
    "\n",
    "    \n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425873f",
   "metadata": {},
   "source": [
    "# Machine Learning Project!\n",
    "\n",
    "Step 4 \n",
    "\n",
    "Goal: This notebook demonstrates the **data preparation** steps for machine learning algorithms.  \n",
    "\n",
    "Content: **Prepare the data**\n",
    "\n",
    "    4.1/ Data Cleaning    \n",
    "    4.2/ Transformation Pipelines (feature scaling, add new feature and impute) \n",
    "___    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde77e9d",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# add more based on the topic of the lab\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# path to the current lab directory - set individually!!!\n",
    "# TODO HERE! \n",
    "PROJECT_DIR = \"./\"\n",
    "if os.path.isdir(PROJECT_DIR): \n",
    "    print('Ok continue.')\n",
    "else: \n",
    "    print('Nok, set correct path to your project directory!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# check the data set dir \n",
    "forest_path = os.path.join(PROJECT_DIR, \"forest_fires\")\n",
    "# print(os.listdir(forest_path))\n",
    "\n",
    "# function to read the csv file \n",
    "def load_local_data(data_path, csv_file):\n",
    "    csv_path = os.path.join(data_path, csv_file)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# load data \n",
    "fires = load_local_data(forest_path, \"forestfires.csv\")\n",
    "\n",
    "# check header and some values \n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291abfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1004f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop labels for training set\n",
    "fires_features = fires.drop(\"area\", axis=1) \n",
    "fires_area = fires[\"area\"].copy()\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbed481",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_features.drop(\"month\", inplace=True, axis=1)\n",
    "fires_features.drop(\"day\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ff7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eee19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I simulate missing values \n",
    "# df.replace('?',np.NaN,inplace=True)\n",
    "fires_features['temp'][4] = np.NaN\n",
    "fires_features['temp'][104] = np.NaN\n",
    "fires_features['temp'][240] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31835be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_features[fires_features.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which records have None / null?\n",
    "\n",
    "sample_incomplete_rows = fires_features[fires_features.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d18348",
   "metadata": {},
   "source": [
    "### Rejecting records with missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43051e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_incomplete_rows.dropna(subset=[\"temp\"])    # option 1\n",
    "# sample_incomplete_rows.drop(\"temp\", axis=1)       # option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37ec87",
   "metadata": {},
   "source": [
    "### Fill-in meadian value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d94367",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = fires_features[\"temp\"].median()\n",
    "sample_incomplete_rows[\"temp\"].fillna(median, inplace=True) # option 3\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the original value \n",
    "print(fires['temp'][4]) \n",
    "print(fires['temp'][104]) \n",
    "print(fires['temp'][240]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39870263",
   "metadata": {},
   "source": [
    "**Scikit-Learn Impoter**\n",
    "\n",
    "Since Scikit-Learn 0.20, the `sklearn.preprocessing.Imputer` class was replaced by the `sklearn.impute.SimpleImputer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\n",
    "except ImportError:\n",
    "    from sklearn.preprocessing import Imputer as SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\", missing_values=np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0888354",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc33731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use this “trained” imputer to transform the training set \n",
    "# by replacing missing values by the learned medians\n",
    "fires_features_imputed = pd.DataFrame(imputer.fit_transform(fires_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f98b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_features_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd0cc0",
   "metadata": {},
   "source": [
    "### Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc77e55",
   "metadata": {},
   "source": [
    "One of the most important transformations you need to apply to your data is feature scaling. Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. \n",
    "\n",
    "**Feature Scaling**: \n",
    "\n",
    "    * min-max scaling (normalization), values are shifted and rescaled so that they end up ranging from 0 to 1. \n",
    "    * standardization: subtracts the mean value, and then it divides by the variance so that the resulting distribution has zero mean and unit variance.\n",
    "\n",
    "Scikit-learn provides `MinMaxScaler` and `StandardScaler` for standardization. \n",
    "\n",
    "The MinMax scaler transformation is given by: \n",
    "\n",
    "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "    X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f81b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_sel = fires[['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e26cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(fires_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b39dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for adding extra features \n",
    "\n",
    "# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\n",
    "RH_ix, wind_ix, = [\n",
    "    list(fires_sel.columns).index(col)\n",
    "    for col in (\"RH\", \"wind\")]\n",
    "\n",
    "print(RH_ix, wind_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined function for transforamation \n",
    "\n",
    "def add_extra_features(X):\n",
    "    RH_per_wind = X[:, RH_ix] / X[:, wind_ix]\n",
    "    return np.c_[X, RH_per_wind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd1921",
   "metadata": {},
   "source": [
    "We can build a pipeline for preprocessing the numerical attributes (use `CombinedAttributesAdder()` or `FunctionTransformer(...)` as preferred):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e94b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_num = fires_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\", missing_values=np.NaN)),\n",
    "        ('attribs_adder', FunctionTransformer(add_extra_features)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# fires_features\n",
    "fires_num_tr = num_pipeline.fit_transform(fires_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fires_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fires_num_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a34264",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fires_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfires_num\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fires_num' is not defined"
     ]
    }
   ],
   "source": [
    "fires_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62699eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fires_num_tr, columns=['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain',\n",
    "        'RHw'], index=fires_num.index).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd8818",
   "metadata": {},
   "source": [
    "You can test also other transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520eeed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pipeline = ColumnTransformer([\n",
    "#         (\"num\", num_pipeline, num_attribs),\n",
    "#         (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "#     ])\n",
    "\n",
    "# df_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256d89a",
   "metadata": {},
   "source": [
    "### Splitting data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff321725",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fires_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# random split \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 4\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mfires_num\u001b[49m\n\u001b[1;32m      5\u001b[0m                                        , test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fires_num' is not defined"
     ]
    }
   ],
   "source": [
    "# random split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(fires_num\n",
    "                                       , test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8017094d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert the selected attributes to Numpy ndarray \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(train_set[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFFMC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDMC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;124m'\u001b[39m]], \n\u001b[1;32m      3\u001b[0m                    dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m      4\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_set[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# convert the selected attributes to Numpy ndarray \n",
    "X_train = np.array(train_set[['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']], \n",
    "                   dtype=np.float64)\n",
    "y_train = np.array(train_set[['area']].values.ravel(), dtype=np.float64) \n",
    "\n",
    "# .values will give the values in a numpy array (shape: (n,1))\n",
    "# .ravel will convert that array shape to (n, ) (i.e. flatten it)\n",
    "#.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfff716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
